---
title: "Time for a tune-up"
image: 'https://raw.githubusercontent.com/PracticumAI/practicumai.github.io/main/images/icons/practicumai_transfer_learning.png'
image-width: 80px
image-height: 80px
layout: full_page
---

Fine-tuning is a more flexible transfer learning technique that involves unfreezing some or all of a pre-trained model’s layers and retraining them on a new dataset. Unlike feature extraction, which keeps most of the model unchanged, fine-tuning allows the model to adapt more specifically to a new task by updating weight parameters.

Fine-tuning is particularly beneficial when the target dataset differs significantly from the original dataset used for pre-training. By adjusting specific layers, fine-tuning enables the model to learn new feature representations while still leveraging its previously acquired knowledge.

## Fine-Tuning Workflow

Here is a rundown of the general steps for performing fine-tuning.

1. **Select a Pre-Trained Model:** Choose a suitable pre-trained model such as ResNet, ViT, or BLIP, depending on the task. As above, choose something close to your target.
1. **Load the Pre-Trained Model:** Use a deep learning framework to load the model with pre-trained weights.
1. **Determine Layers to Unfreeze:** Identify and selectively unfreeze layers that should be retrained based on the similarity between the original and new tasks.
1. **Modify the Output Layers:** Adjust the model’s output layers to match the prediction target of the new dataset.
1. **Adjust Learning Rates:** Apply a lower learning rate to pre-trained layers and a higher learning rate to newly initialized layers to prevent overwriting learned representations.
1. **Preprocess the Input Data:** Ensure images are sized, normalized, and formatted to match model requirements.
1. **Train the Model:** Perform training with backpropagation, monitoring validation loss to prevent overfitting.
1. **Evaluate and Optimize:** Use performance metrics such as accuracy and F1 score, and apply hyperparameter tuning if needed.

---

[Return to Module 2](02_implementing_tl_techniques.md) or [Continue to My friend, LoRA](02.3_my_friend_lora.md)