---
title: "The Tools for AI"
image: 'https://raw.githubusercontent.com/PracticumAI/practicumai.github.io/main/images/icons/practicumai_deep_learning.png'
image-width: 80px
image-height: 80px
layout: full_page_no_title
---

![How to tune your models banner](/images/dlf_how_to_tune_banner.png)

## Topics: The following topics are covered in this module:

* Gradient Descent
* Transfer Learning
* Underfitting and Overfitting
* Fine Tuning

## Objectives

By the end of this module, students will be able to:

1. Describe the purpose and process of gradient descent.
1. Discuss the error loss function.
1. Describe optimizers.
1. Recognize signs of underfitting and overfitting.
1. Experiment with hyperparameter tuning.

## Watch

**Video coming soon**

## Topics

* [Understanding Gradient Descent](/deep_learning/03.1_gradient_descent/)
* [Overfitting and Underfitting](/deep_learning/03.2_overfitting_underfitting/)
* [Transfer Learning & Fine Tuning](/deep_learning/03.3_transfer_learning/)

## Hands-on Exercise

Through this exercise, you'll gain hands-on experience applying deep learning to solve another image recognition task. As discussed above, the loss function, optimizer, and more are all hyperparameters that can be adjusted to try to train a better model. You will get some experience with hyperparameter optimization.

Work on notebook **03_bees_vs_wasps.ipynb**.

## Conclusion

In this module, we explored essential concepts and techniques in machine learning optimization:

1. **Basics of Gradient Descent**: Introduced the algorithm as a method for minimizing loss functions in machine learning models.
1. **Loss Functions**: Discussed various types, such as Mean Squared Error and Cross-Entropy, highlighting their role in model accuracy.
1. **Optimizers**: Covered different optimizers like Stochastic Gradient Descent (SGD) and Adam, focusing on their application in refining the learning process.
1. **Transfer Learning & Fine Tuning**: Touched on these important, advanced deep learning techniques.
1. **Deep Learning Implementation**: Explored hyperparameter optimization to train better models.

This module provided a foundational understanding of how gradient descent drives the learning process in machine learning and deep learning models.

